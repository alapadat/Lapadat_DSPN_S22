{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szkkhiCZDF52"
   },
   "source": [
    "# Exercise 8:  Linear models, continued\n",
    "\n",
    "This homework assignment is designed to give you a deeper understanding of linear models. First, we'll dive into the math behind the closed-form solution of maximum likelihood estimation. **In the first section below, write your answers using Latex equation formatting.**\n",
    "\n",
    "*Note: Check out [this page](https://gtribello.github.io/mathNET/assets/notebook-writing.html) and [this page](https://towardsdatascience.com/write-markdown-latex-in-the-jupyter-notebook-10985edb91fd) for resources on how to do Latex formatting. You can also double click on the question cells in this notebook to see how math is formatted in the questions.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJscNReoylRt"
   },
   "source": [
    "---\n",
    "## 1. Deriving the Maximum Likelihood Estimate for Simple Linear Regression (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nH82gwuymPi0"
   },
   "source": [
    "Using the mean squared error (MSE) as your objective function (the thing you're trying to minimize when you fit your model) allows for a closed form solution to finding the maximum likelihood estimate (MLE) of your model parameters in linear regression. Letâ€™s consider the simple, single predictor variable model, i.e. simple linear regression :  $Y= \\beta_0 + \\beta_1 X $. \n",
    "\n",
    "a) Use algebra to show how you can expand out $MSE(\\beta_0, \\beta_1)$ to get from i to ii below.\n",
    "\n",
    "> _i)_ $E[ (Y-(\\beta_0 + \\beta_1 X))^2]$\n",
    "\n",
    "> _ii)_ $E[Y^2] -2 \\beta_0E[Y]-2 \\beta_1 Cov[X,Y]-2 \\beta_1 E[X]E[Y]+ \\beta_0^2 +2 \\beta_0 \\beta_1 E[X]+\\beta_1^2 Var[X]+ \\beta_1^2 (E[X])^2$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dn2hveNho-Of"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "> _i)_ $E[ (Y-(\\beta_0 + \\beta_1 X))^2]$\n",
    "\n",
    "> _ii)_ $E[Y^2] -2 \\beta_0E[Y]-2 \\beta_1 Cov[X,Y]-2 \\beta_1 E[X]E[Y]+ \\beta_0^2 +2 \\beta_0 \\beta_1 E[X]+\\beta_1^2 Var[X]+ \\beta_1^2 (E[X])^2$\n",
    "\n",
    "\n",
    "Step 1: square the equation within the brackets. \n",
    "\n",
    "> _1)_ $E[ (Y-(\\beta_0 + \\beta_1 X))^2]$ \n",
    "\n",
    ">_2)_ $E[ Y^2 - 2 \\beta_0 \\beta_1 XY + (\\beta_0 + \\beta_1 X)^2 ]$\n",
    "\n",
    "Step 2: The EV of a function, once its terms are multiplied out, can be applied independently to each term, according to Tip 2. Pullled -1 out of the term B preemptively, according to Tip 3.\n",
    "\n",
    ">_3)_ $E[Y^2] + -1 * E[2 \\beta_0 + \\beta_1 XY] + E[(\\beta_0 + \\beta_1 X)^2]$\n",
    "\n",
    "Step 3: To prevent excessive dumbassery, we will now deal with terms A, B, and C separately, and then add them together.\n",
    "\n",
    "Step 3-1: The expected value of a random variable squared is the same as the squared expected value of that variable, plus its variance (Tip 4). \n",
    "\n",
    ">_1)_ $E[Y^2]$\n",
    "\n",
    ">_2)_ $= Var(Y) + E[Y]^2$\n",
    "\n",
    "Step 3-2: \n",
    "\n",
    ">_1)_ $-1 * E[2 \\beta_0 + \\beta_1 XY]$\n",
    "\n",
    "Step 3-2-2: Tip 3, pull out coefficients.\n",
    "\n",
    ">_2)_ $= -2 * E[\\beta_0 + \\beta_1 XY]$\n",
    "\n",
    "Step 3-2-3: Algebra, unfortunately.\n",
    "\n",
    ">_3)_ $= -2 * E[(\\beta_0 + \\beta_1 X(Y)]$\n",
    "\n",
    ">_4)_ $= -2 * E[(\\beta_0XY + \\beta_1XY]$\n",
    "\n",
    ">_5)_ $= -2 *  E[(\\beta_0XY] + E[\\beta_1XY]$\n",
    "\n",
    "Step 3-2-4: Pull out coefficients, tip 3\n",
    "\n",
    ">_6)_ $= -2 *  \\beta_0 * E[XY] + \\beta_1 * E[XY]$\n",
    "\n",
    "Step 3-2-5: The expected value of the product of two random variables is the covariance of those variables plus the product of their expected values, Tip 5.\n",
    "\n",
    ">_6)_ $= -2 *  \\beta_0 * Cov(X,Y) + E[X]E[Y] + \\beta_1 * Cov(X,Y) + E[X]E[Y]$\n",
    "\n",
    "Step 3-2-6: Putting humpty dumpty back together again.\n",
    "\n",
    ">_7)_ $= -2 *  \\beta_0Cov(X,Y) + \\beta_0E[X]E[Y] + \\beta_1Cov(X,Y) + \\beta_1E[X]E[Y]$\n",
    "\n",
    ">_8)_ $= -2\\beta_0Cov(X,Y) - 2\\beta_0E[X]E[Y] - 2\\beta_1Cov(X,Y) - 2\\beta_1E[X]E[Y]$\n",
    "\n",
    "Step 3-3: Just when you thought you were done...more algebra. \n",
    "\n",
    ">_1)_ $ E[(\\beta_0 + \\beta_1 X)^2 $\n",
    "\n",
    ">_2)_ $E[\\beta_0^2 + 2\\beta_0\\beta_1X + \\beta_1^2X^2]$\n",
    "\n",
    ">_3)_ $ E[\\beta_0^2] + E[2\\beta_0\\beta_1X] + E[\\beta_1^2X^2] $\n",
    "\n",
    "Step 3-3-1: Tip 1 to term 1\n",
    "\n",
    ">_1)_ $ \\beta_0^2 + E[2\\beta_0\\beta_1X] + E[\\beta_1^2X^2] $ \n",
    "\n",
    "Step 3-3-2: Tip 3 to term 2, 3\n",
    "\n",
    ">_2)_ $ \\beta_0^2 + 2\\beta_0\\beta_1E[X] + E[\\beta_1^2X^2] $ \n",
    "\n",
    ">_3)_ $ \\beta_0^2 + 2\\beta_0\\beta_1E[X] + \\beta_1^2E[X]^2 $\n",
    "\n",
    "Step  3-3-3: Tip 4 to term 3\n",
    "\n",
    ">_3)_ $ \\beta_0^2 + 2\\beta_0\\beta_1E[X] + \\beta_1^2 Var[X] + \\beta_1^2(E[X])^2 $\n",
    "\n",
    "Step 4: Adding all our terms\n",
    "\n",
    "$Var(Y) + E[Y]^2 - 2\\beta_0Cov(X,Y) - 2\\beta_0E[X]E[Y] - 2\\beta_1Cov(X,Y) - 2\\beta_1E[X]E[Y] + \\beta_0^2 + 2\\beta_0\\beta_1E[X] + \\beta_1^2E[X]^2 $\n",
    "\n",
    "= $E[Y^2] - 2\\beta_0Cov(X,Y) - 2\\beta_0E[X]E[Y] - 2\\beta_1Cov(X,Y) - 2\\beta_1E[X]E[Y] + \\beta_0^2 + 2\\beta_0\\beta_1E[X] + \\beta_1^2E[X]^2 $$\n",
    " \n",
    "The answers are slightly different, I think I messed up in step 3-2-5/\n",
    " \n",
    "$E[Y^2] -2 \\beta_0E[Y]-2 \\beta_1 Cov[X,Y]-2 \\beta_1 E[X]E[Y]+ \\beta_0^2 +2 \\beta_0 \\beta_1 E[X]+\\beta_1^2 Var[X]+ \\beta_1^2 (E[X])^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCr46r9xwRXP"
   },
   "source": [
    "b) Prove that the MLE of $\\beta_0$ is $E[Y]- \\beta_1 E[X]$ by taking the derivative of _ii_ above, with respect to $\\beta_0$, setting the derivative to zero, and solving for $\\beta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ul-PZyLbwTCQ"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    ">_1)_ $E[Y^2] -2 \\beta_0E[Y]-2 \\beta_1 Cov[X,Y]-2 \\beta_1 E[X]E[Y]+ \\beta_0^2 +2 \\beta_0 \\beta_1 E[X]+\\beta_1^2 Var[X]+ \\beta_1^2 (E[X])^2$ \n",
    "\n",
    "Step 1: Remove all terms that do not have $\\beta_0$\n",
    "\n",
    ">_1)_ $ -2\\beta_0E[Y] + \\beta_0^2 + 2\\beta_0\\beta_1E[X] $\n",
    "\n",
    "Step 2: Divide (both sides) by 2.\n",
    "\n",
    ">_1)_ $ 1/2 * 0 = (-2\\beta_0E[Y] + \\beta_0^2 + 2\\beta_0\\beta_1E[X]) * 1/2 $\n",
    "\n",
    ">_2)_ $= -E[Y] + \\beta_0 + \\beta_1E[X]$\n",
    "\n",
    "Step 3: Isolate $\\beta_0$\n",
    "\n",
    ">_1)_ $(-\\beta_0) + 0= -E[Y] + \\beta_0 + \\beta_1E[X] + (-\\beta_0)$\n",
    "\n",
    ">_2)_ $ -1 * (-\\beta_0) = -1 * (-E[Y] + \\beta_1E[X]) $\n",
    "\n",
    ">_3)_ $ \\beta_0 = E[Y] - \\beta_1E[X] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uv4Z7afw4gB"
   },
   "source": [
    "c) Prove that the MLE for $\\beta_1$ is $Cov[X,Y]/Var[X]$ by taking the derivative of equation _ii_ above, with respect to $\\beta_1$, setting the derivative to zero, and solving for $\\beta_1$. *Hint: after you've simplified / expanded a bit, plug in the solution for $\\beta_0$ from part b.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWTFZ6ZSw6sh"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    ">_1)_ $E[Y^2] -2 \\beta_0E[Y]-2 \\beta_1 Cov[X,Y]-2 \\beta_1 E[X]E[Y]+ \\beta_0^2 +2 \\beta_0 \\beta_1 E[X]+\\beta_1^2 Var[X]+ \\beta_1^2 (E[X])^2$ \n",
    "\n",
    "Step 1) Remove all terms that don't include $\\beta_1$\n",
    "\n",
    ">_1)_ $-2 \\beta_1 Cov[X,Y]-2 \\beta_1 E[X]E[Y]+2 \\beta_0 \\beta_1 E[X]+\\beta_1^2 Var[X]+ \\beta_1^2 (E[X])^2$ \n",
    "\n",
    "Step 2) d/dx of term A\n",
    "\n",
    ">_1)_ $ \\beta_1 Cov[X,Y] $\n",
    "\n",
    ">_2)_ $ -2 (\\beta_1 Cov[X,Y]) $\n",
    "\n",
    "Step 2) d/dx of term B\n",
    "\n",
    ">_1)_ $ -2 \\beta_1 E[X]E[Y] $\n",
    "\n",
    ">_2)_ $ -2 (\\beta_1 E[X]E[Y]) $\n",
    "\n",
    "Step 3) d/dx of term C\n",
    "\n",
    ">_1)_ $ 2 (\\beta_0 \\beta_1 E[X]) $\n",
    "\n",
    "Step 3-1) plug in $ \\beta_0 = E[Y] - \\beta_1E[X] $\n",
    "\n",
    ">_1)_ $ 2 (E[Y] - \\beta_1E[X]) (\\beta_1 E[X]) $\n",
    "\n",
    ">_2)_ $ 2 ((\\beta_1 E[Y]E[X]) + -1 (\\beta_1E[X])^2) $\n",
    "\n",
    "Step 3-1 alt ver, taking $\\beta_1 $out via derivative\n",
    "\n",
    ">_1)_ $ 2 (\\beta_0 (1) E[X]) $\n",
    "\n",
    "Step 3-2, taking $\\beta_0 $ out by substitutiong $\\beta_0 = E[Y] - \\beta_1E[X]$\n",
    "\n",
    ">_1)_ $ 2 ( (E[Y] - \\beta_1E[X]) (E[X]) $\n",
    "\n",
    "Step 4) d/dx of term D\n",
    "\n",
    ">_1)_ $ \\beta_1^2 Var[X] $\n",
    "\n",
    ">_2)_ $ 2\\beta_1 Var[X] $\n",
    "\n",
    "Step 5) d/dx of term E\n",
    "\n",
    ">_1)_ $ \\beta_1^2 (E[X])^2 $\n",
    "\n",
    ">_2)_ $ 2\\beta_1 (E[X])^2 $\n",
    "\n",
    "Step 6) Humpty dumpty\n",
    "\n",
    "$ 0 = -2 (\\beta_1 Cov[X,Y]) + -2 (\\beta_1 E[X]E[Y]) + 2 * (E[X](E[Y] - \\beta_1E[X])) + 2\\beta_1 Var[X] + 2\\beta_1 (E[X])^2$\n",
    "\n",
    "$ = -2 (\\beta_1 Cov[X,Y]) + -2 (\\beta_1 E[X]E[Y]) + 2 * (E[X](E[Y] - \\beta_1E[X])) + 2\\beta_1 Var[X] + 2\\beta_1 (E[X])^2$ $/2$\n",
    "\n",
    "$ = -(\\beta_1 Cov[X,Y]) + -(\\beta_1 E[X]E[Y]) + E[X](E[Y] - \\beta_1E[X]) + \\beta_1 Var[X] + \\beta_1 (E[X])^2$\n",
    "\n",
    "Step 6-1) Cancel out $ (E[X])^2 $\n",
    "\n",
    "$ = -(\\beta_1 Cov[X,Y]) + -(\\beta_1 E[X]E[Y]) + E[X]E[Y] - \\beta_1E[X]^2 + \\beta_1 Var[X] + \\beta_1 (E[X])^2$\n",
    "\n",
    "$ = -(\\beta_1 Cov[X,Y]) + -(\\beta_1 E[X]E[Y]) + E[X]E[Y] + \\beta_1 Var[X]$\n",
    "\n",
    "Step 6-1) Isloate $ \\beta_1 $\n",
    "\n",
    "$ -E[X]E[Y] = -(\\beta_1 Cov[X,Y]) + -(\\beta_1 E[X]E[Y])  + \\beta_1 Var[X]$\n",
    "\n",
    "Step 6-2) divide by $\\beta_1 $\n",
    "\n",
    "$ -E[X]E[Y] / \\beta_1  = -(\\beta_1 Cov[X,Y]) + -(\\beta_1 E[X]E[Y])  + \\beta_1 Var[X] / \\beta_1 $\n",
    "\n",
    "$ -E[X]E[Y] / \\beta_1 = -(Cov[X,Y]) + -(E[X]E[Y])  +  Var[X]$\n",
    "\n",
    "Step 6-3) divide by $ -E[X]E[Y] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66X264ZpDF58"
   },
   "source": [
    "---\n",
    "## 2. Connecting to data (4 points)\n",
    "\n",
    "Now let's connect this to some real data. Once again we'll be using the  **unrestricted_trimmed_1_7_2020_10_50_44.csv** file from the *Homework/hcp_data* folder in the class GitHub repository. \n",
    "\n",
    "â€‹\n",
    "This data is a portion of the [Human Connectome Project database](http://www.humanconnectomeproject.org/). It provides measures of cognitive tasks and brain morphology measurements from 1206 participants. The full description of each variable is provided in the **HCP_S1200_DataDictionary_April_20_2018.csv** file in the *Homework/hcp_data* folder in the class GitHub repository. \n",
    "\n",
    "a) Use the `setwd` and `read.csv` functions to load data from the **unrestricted_trimmed_1_7_2020_10_50_44.csv** file. Then use the `tidyverse` tools make a new dataframe `d1` that only inclues the subject ID (`Subject`), Flanker Task performance (`Flanker_Unadj`), and total grey matter volume (`FS_Total_GM_Vol`) variables and remove all _NA_ values.\n",
    "\n",
    "Use the `head` function to look at the first few rows of each data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZ0lngBjDF58"
   },
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3owDQ0U2Ewn"
   },
   "source": [
    "b) Now we're going to see if the solutions we proved above actually line up with the model fit that R gives us (it should...). Calculate what the $\\beta_0$ and $\\beta_1$ coefficients should be for a simple linear regression model using `Flanker_Unadj` as $Y$ and `FS_Total_GM_Vol` as $X$. Use the formulas we derived above ($\\beta_1 = Cov[XY]/Var[X]$ , $\\beta_0 = E[Y] - \\beta_1E[X]$). Then use `lm()` to compare the coefficients you calculated with the ones R gives you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWvD8shRDF5_"
   },
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcnXbsZvDF6B"
   },
   "source": [
    "**DUE:** 5pm EST, March 1, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aG5swCweDF6B"
   },
   "source": [
    "**IMPORTANT** Did you collaborate with anyone on this assignment? If so, list their names here. \n",
    "> *Someone's Name*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
